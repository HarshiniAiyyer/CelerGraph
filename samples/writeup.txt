This is the **Ultimate CelerGraph Walkthrough Script**.

I have merged the high-level narrative (The "Why") with the technical deep dive (The "How"). This script tells a complete story: starting with the problem of standard RAG, moving through the architecture and code implementation, and finishing with the observability and live demo.

**Estimated Duration:** 12–15 Minutes.

---

# **Title: CelerGraph – The Architecture of Code-Aware RAG**

### **Part 1: The Problem & The Solution (0:00 - 2:00)**

**[Visual: Split Screen. Left: A standard "Vector Cloud" (dots in space). Right: A complex Python file with imports and class inheritance.]**

**Speaker:**
"Welcome back. If you’ve ever tried to build a RAG system for a codebase using *only* vector databases, you know the pain. You ask, 'How does dependency injection work?' and the Vector DB gives you five random snippets that mention the word 'dependency.'

But it misses the context. It doesn't know that Function A *calls* Function B, or that this Class *inherits* from that Base Model. Code isn't just text—it’s a structure. It’s a graph.

**[Visual: The Vector Cloud fades out, replaced by a Neo4j Knowledge Graph visualization connecting nodes.]**

**Speaker:**
"That is why we built **CelerGraph**. It’s a GraphRAG system specifically designed to map the FastAPI core codebase. We aren't just embedding text; we are mapping the relationships between modules, classes, and functions.

Today, we are going to walk through the entire stack. We’ll look at the **SOLID principles** we used to architect it, the **LibCST** parsing logic that extracts the graph, and the **Arize Phoenix** observability layer that keeps it honest."

---

### **Part 2: The Architecture & Stack (2:00 - 4:00)**

**[Visual: Show the Architecture Diagram from the documentation. Highlight 'FastAPI', 'Neo4j', and 'Groq'.]**

**Speaker:**
"First, let’s look at the high-level architecture.

1.  **The Brain:** We are using **LangChain** orchestrating **GPT-OSS-120b** via **Groq**. We chose Groq for speed—when you’re doing graph traversals *and* vector lookups, you can't afford a slow LLM.
2.  **The Storage:** We use a hybrid approach. **ChromaDB** handles the vector embeddings for fuzzy search. **Neo4j** handles the structural knowledge graph.
3.  **The API:** The whole thing is wrapped in **FastAPI** for async performance.

**[Visual: Transition to VS Code showing the file tree `d:\inco\`.]**

**Speaker:**
"But a stack isn't an architecture. To keep this maintainable, we structured the project using **SOLID principles**.

If you look at `graphrag_solid.py`, you’ll see we use **Dependency Injection**. The system doesn't create its own database connections. It receives a configuration object and uses a **Factory Pattern** (defined in `core/factories.py` and creating services implemented in `core/services.py`) to spin up services. This ensures our business logic is decoupled from the specific tools we use."

---

### **Part 3: Deep Dive – The Ingestion Pipeline (4:00 - 7:00)**

**[Visual: Open `graph_indexing/kgbuild/runner.py`. Highlight the `extract_project` function.]**

**Speaker:**
"Now, let's get into the code. How do we actually turn a Python file into a Graph?

We don't use regex. It's too brittle. In `graph_indexing/kgbuild/runner.py`, we built an ingestion pipeline that uses **LibCST** (Concrete Syntax Tree).

**[Visual: Switch to `graph_indexing/kgbuild/python_extractor.py`. Highlight the `PyExtract` class.]**

**Speaker:**
"In `python_extractor.py`, we implemented a `CSTVisitor` class called `PyExtract`. This allows us to walk through the code syntax tree node by node.

Look at the `visit_ClassDef` method here.
When the parser hits a class definition, we don't just grab the name.
1.  We create a `CONTAINS` edge from the module to the class.
2.  Crucially, we iterate through `node.bases` to find what the class inherits from.
3.  We immediately create an `INHERITS` edge in the graph.

**[Visual: Scroll down to `visit_Call` in `python_extractor.py`.]**

**Speaker:**
"We do the same for function calls in `visit_Call`. If Function A calls Function B, we resolve that relationship and create a `CALLS` edge. This is how the system knows the execution flow before we even turn on the LLM."

---

### **Part 4: Deep Dive – Hybrid Retrieval (7:00 - 9:30)**

**[Visual: Open `core/graphrag.py`. Focus on `parallel_retrieval`.]**

**Speaker:**
"Once the data is ingested, how do we retrieve it?
In `core/graphrag.py`, we use a **Hybrid Retrieval Strategy**.

We use LangChain's LCEL to run parallel operations. We query the `node_embeddings` and `code_chunks` collections simultaneously using `RunnableParallel`.

**[Visual: Highlight the `expand_graph` function in `graphrag.py`.]**

**Speaker:**
"But here is the secret sauce: the `expand_graph` function.
We take the IDs of the nodes we found via vector search, and we ask Neo4j to 'expand' them.

Look at this Cypher query: `MATCH (n)-[*1..depth]-(m)`.
This pulls in the neighbors. If vector search finds a function, this query pulls in the class that holds it and the other functions that call it. This gives the LLM the full structural context it needs to answer complex architectural questions."

---

### **Part 5: Repository Pattern & Semantic Caching (9:30 - 11:00)**

**[Visual: Open `core/retrieval.py`.]**

**Speaker:**
"To keep the code clean, we implemented the **Repository Pattern** in `core/retrieval.py`.
The main application doesn't need to know *how* to talk to ChromaDB. It just calls `retrieve_similar_nodes`.

**[Visual: Highlight the `SemanticCache` logic in `graphrag.py` or `retrieval.py`.]**

**Speaker:**
"We also implemented **Semantic Caching**. LLMs are expensive and slow. Before we ever hit the LLM, we check our cache in Chroma.
Because we use vector similarity for the cache, 'How does routing work?' and 'Explain the routing mechanism' will hit the same cache entry, saving us money and time."

---

### **Part 6: Observability (11:00 - 13:00)**

**[Visual: Switch to `observability/tracing/instrumentation.py`.]**

**Speaker:**
"Now, a system this complex is hard to debug. If the answer is wrong, was it the retrieval? The graph expansion? Or the LLM hallucinating?

To solve this, we integrated **Arize Phoenix**.
We used the **Observer Pattern** via Python decorators. In `instrumentation.py`, we defined a `@trace_span` decorator.

**[Visual: Switch to `retrieval.py` to show the decorator usage.]**

**Speaker:**
"We simply apply `@trace_span` to our critical functions, like `retrieve_similar_nodes` in `retrieval.py`. We don't clutter the logic with logging code.

**[Visual: Cut to the Phoenix Dashboard (Web UI) showing a waterfall trace.]**

**Speaker:**
"This generates traces like this. We can see the exact moment the Graph lookup happened, what nodes Neo4j returned, and how long it took. If the retrieval feels slow, we know exactly which span is the bottleneck."

---

### **Part 7: The Demo (13:00 - 14:30)**

**[Visual: Terminal window executing `run_all.bat`. Then switch to Browser localhost:5173.]**

**Speaker:**
"Let's see it live. I’m running `run_all.bat` to spin up the backend, frontend, and Phoenix server.

I’ll ask: *'Explain the middleware execution order.'*

**[Visual: The answer generates on screen. The mouse highlights a citation `[node: middleware]`.]**

**Speaker:**
"It retrieved the answer. But look at the references. It didn't just guess; it pulled the specific nodes from the `applications.py` and `middleware` modules. It combined the vector similarity with the graph relationship to give us a complete answer."

---

### **Part 8: Conclusion (14:30 - End)**

**[Visual: Camera returns to Speaker or Logo.]**

**Speaker:**
"So that is CelerGraph. It is a full-stack RAG application that respects the structure of code.
* **FastAPI** for the async backend.
* **LibCST** for deep code extraction.
* **Neo4j + Chroma** for hybrid storage.
* And **Phoenix** to keep eyes on the system.

The code is available in the repository. I encourage you to clone it, run the installer, and start chatting with your code.

Thanks for watching."